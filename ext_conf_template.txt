# cat=general; type=options[gemini,openai,ollama,claude,mistral,cohere,deepl,googletranslate,custom]; label=AI Provider: Select the AI provider to use.
provider = gemini

# cat=general; type=int; label=AI max retry attempts.
maxRetries = 5

# cat=Gemini; type=string; label=Gemini API Key: Your Google Gemini API key.
gemini.apiKey =

# cat=Gemini; type=string; label=Gemini Model Name: The Gemini model to use.
gemini.modelName = gemini-2.0-flash

# cat=Gemini; type=float; label=Gemini Temperature: The temperature for the Gemini model.
gemini.temperature = 0.9

# cat=Gemini; type=float; label=Gemini TopP: The topP for the Gemini model.
gemini.topP = 0.95

# cat=Gemini; type=int; label=Gemini TopK: The topK for the Gemini model.
gemini.topK = 40

# cat=Gemini; type=int; label=Gemini Candidate Count: The candidate count for the Gemini model.
gemini.candidateCount = 1

# cat=Gemini; type=int; label=Gemini Max Output Tokens: The max output tokens for the Gemini model.
gemini.maxOutputTokens = 1024

# cat=Gemini; type=string; label=Gemini Stop Sequences: The stop sequences for the Gemini model (comma separated).
gemini.stopSequences =

# cat=Gemini; type=int; label=Gemini Chunk Size: The chunk size when using stream mode.
gemini.chunkSize = 100

# cat=Gemini; type=string; label=Gemini Fallback Models: Comma-separated list of Gemini models to use as fallback.
gemini.fallbackModels = gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash

# cat=Gemini; type=int; label=Gemini Max Input Tokens Allowed: The max input tokens allowed for the Gemini model.
gemini.maxInputTokensAllowed = 1000000

# cat=OpenAi; type=string; label=OpenAI API Key: Your OpenAI API key.
openai.apiKey =

# cat=OpenAi; type=string; label=OpenAI Model Name: The OpenAI model to use.
openai.modelName = gpt-4o

# cat=OpenAi; type=float; label=OpenAI Temperature: The temperature for the OpenAI model.
openai.temperature = 1.0

# cat=OpenAi; type=float; label=OpenAI TopP: The topP for the OpenAI model.
openai.topP = 1.0

# cat=OpenAi; type=int; label=OpenAI Max Output Tokens: The maximum number of tokens to generate.
openai.maxOutputTokens = 1024

# cat=OpenAi; type=string; label=OpenAI Stop: The stop sequences for the OpenAI model (comma separated).
openai.stop =

# cat=OpenAi; type=int; label=OpenAI Chunk Size: The chunk size when using stream mode.
openai.chunkSize = 100

# cat=OpenAi; type=string; label=OpenAI Fallback Models: Comma-separated list of OpenAI models to use as fallback.
openai.fallbackModels =

# cat=OpenAi; type=int; label=OpenAI Max Input Tokens Allowed: The max input tokens allowed for the OpenAI model.
openai.maxInputTokensAllowed = 128000

# cat=Claude; type=string; label=Claude API Key: Your Claude API key.
claude.apiKey =

# cat=Claude; type=string; label=Claude Model Name: The Claude model to use.
claude.modelName = claude-opus-4-1-20250805

# cat=Claude; type=string; label=Claude API Version: The Claude API version to use.
claude.apiVersion = 2023-06-01

# cat=Claude; type=int; label=Claude Max Tokens: The maximum number of tokens to generate.
claude.maxTokens = 1024

# cat=Claude; type=string; label=Claude System Prompt: The system prompt for the Claude model.
claude.system =

# cat=Claude; type=string; label=Claude Stop Sequences: The stop sequences for the Claude model (comma separated).
claude.stopSequences =

# cat=Claude; type=float; label=Claude Temperature: The temperature for the Claude model.
claude.temperature = 1.0

# cat=Claude; type=float; label=Claude TopP: The topP for the Claude model.
claude.topP = 1.0

# cat=Claude; type=int; label=Claude TopK: The topK for the Claude model.
claude.topK = 5

# cat=Claude; type=int; label=Claude Chunk Size: The chunk size when using stream mode.
claude.chunkSize = 100

# cat=Claude; type=string; label=Claude Fallback Models: Comma-separated list of Claude models to use as fallback.
claude.fallbackModels = claude-opus-4-1-20250805, claude-opus-4-20250514, claude-sonnet-4-20250514, claude-3-7-sonnet-20250219, claude-3-5-haiku-20241022, claude-3-5-haiku-20241022

# cat=Claude; type=int; label=Claude Max Input Tokens Allowed: The max input tokens allowed for the Claude model.
claude.maxInputTokensAllowed = 200000

# cat=Mistral; type=string; label=Mistral API Key: Your Mistral API key.
mistral.apiKey =

# cat=Mistral; type=string; label=Mistral Model Name: The Mistral model to use.
mistral.modelName = mistral-large-latest

# cat=Mistral; type=float; label=Mistral Temperature: The temperature for the Mistral model.
mistral.temperature = 0.7

# cat=Mistral; type=float; label=Mistral TopP: The topP for the Mistral model.
mistral.topP = 1.0

# cat=Mistral; type=int; label=Mistral Max Tokens: The maximum number of tokens to generate.
mistral.maxTokens = 1024

# cat=Mistral; type=string; label=Mistral Stop: The stop sequences for the Mistral model (comma separated).
mistral.stop =

# cat=Mistral; type=int; label=Mistral Random Seed: The random seed for the Mistral model.
mistral.randomSeed = 0

# cat=Mistral; type=boolean; label=Mistral Safe Prompt: Enable safe prompt for Mistral API.
mistral.safePrompt = 0

# cat=Mistral; type=int; label=Mistral Chunk Size: The chunk size when using stream mode.
mistral.chunkSize = 100

# cat=Mistral; type=string; label=Mistral Fallback Models: Comma-separated list of Mistral models to use as fallback.
mistral.fallbackModels =

# cat=Mistral; type=int; label=Mistral Max Input Tokens Allowed: The max input tokens allowed for the Mistral model.
mistral.maxInputTokensAllowed = 32768

# cat=Cohere; type=string; label=Cohere API Key: Your Cohere API key.
cohere.apiKey =

# cat=Cohere; type=string; label=Cohere Model Name: The Cohere model to use.
cohere.modelName = command-a-03-2025

# cat=Cohere; type=int; label=Cohere Max Tokens: The maximum number of tokens to generate.
cohere.maxTokens = 1024

# cat=Cohere; type=float; label=Cohere Temperature: The temperature for the Cohere model.
cohere.temperature = 0.3

# cat=Cohere; type=float; label=Cohere P: The P for the Cohere model.
cohere.p = 0.75

# cat=Cohere; type=int; label=Cohere K: The K for the Cohere model.
cohere.k = 0

# cat=Cohere; type=float; label=Cohere Frequency Penalty: The frequency penalty for the Cohere model.
cohere.frequencyPenalty = 0.0

# cat=Cohere; type=float; label=Cohere Presence Penalty: The presence penalty for the Cohere model.
cohere.presencePenalty = 0.0

# cat=Cohere; type=string; label=Cohere Stop Sequences: The stop sequences for the Cohere model (comma separated).
cohere.stopSequences =

# cat=Cohere; type=string; label=Cohere Preamble: The preamble for the Cohere model.
cohere.preamble =

# cat=Cohere; type=int; label=Cohere Chunk Size: The chunk size when using stream mode.
cohere.chunkSize = 100

# cat=Cohere; type=string; label=Cohere Fallback Models: Comma-separated list of Cohere models to use as fallback.
cohere.fallbackModels = command-a-03-2025, command-r7b-12-2024, command-a-reasoning-08-2025, command-a-vision-07-2025, command-r-08-2024, command-r-plus-08-2024

# cat=Cohere; type=int; label=Cohere Max Input Tokens Allowed: The max input tokens allowed for the Cohere model.
cohere.maxInputTokensAllowed = 128000

# cat=DeepL; type=string; label=DeepL API Key: Your DeepL API key.
deepl.apiKey =

# cat=DeepL; type=string; label=DeepL Source Language: The source language for DeepL (optional).
deepl.sourceLang =

# cat=DeepL; type=options[0,1,nonewlines]; label=DeepL Split Sentences: How sentences should be split.
deepl.splitSentences = 1

# cat=DeepL; type=boolean; label=DeepL Preserve Formatting: Preserve formatting in the translation.
deepl.preserveFormatting = 0

# cat=DeepL; type=options[less,more,default]; label=DeepL Formality: Desired formality for the translation.
deepl.formality = default

# cat=DeepL; type=string; label=DeepL Glossary ID: ID of a glossary to use.
deepl.glossaryId =

# cat=DeepL; type=options[xml,html]; label=DeepL Tag Handling: Type of tags to handle.
deepl.tagHandling =

# cat=DeepL; type=boolean; label=DeepL Outline Detection: Detect XML structure.
deepl.outlineDetection = 0

# cat=DeepL; type=string; label=DeepL Non Splitting Tags: Comma-separated list of tags that should not be split.
deepl.nonSplittingTags =

# cat=DeepL; type=options[pro,free]; label=DeepL API Version: Select the DeepL API version to use (pro or free).
deepl.apiVersion = free

# cat=Ollama; type=string; label=Ollama Model Name: The Ollama model to use (e.g., llama3).
ollama.modelName = llama3

# cat=Ollama; type=string; label=Ollama endPoint Url: The endPoint Url to use.
ollama.endPoint = http://ollama:11434

# cat=Ollama; type=float; label=Ollama Temperature: The temperature for the Ollama model.
ollama.temperature = 0.8

# cat=Ollama; type=float; label=Ollama TopP: The topP for the Ollama model.
ollama.topP = 0.9

# cat=Ollama; type=int; label=Ollama Num Predict: The number of tokens to predict for the Ollama model.
ollama.numPredict = 1024

# cat=Ollama; type=string; label=Ollama Stop: The stop sequences for the Ollama model (comma separated).
ollama.stop =

# cat=Ollama; type=string; label=Ollama Format: The format for the Ollama model (e.g., json).
ollama.format =

# cat=Ollama; type=string; label=Ollama System: The system prompt for the Ollama model.
ollama.system =

# cat=Ollama; type=int; label=Ollama Chunk Size: The chunk size when using stream mode.
ollama.chunkSize = 100

# cat=Ollama; type=string; label=Ollama Fallback Models: Comma-separated list of Ollama models to use as fallback.
ollama.fallbackModels =

# cat=Ollama; type=int; label=Ollama Max Input Tokens Allowed: The max input tokens allowed for the Ollama model.
ollama.maxInputTokensAllowed = 8192

# cat=GoogleTranslate; type=string; label=Google Translate API Key: Your Google Translate API key.
googleTranslate.apiKey =

# cat=GoogleTranslate; type=string; label=Google Translate Target Language: The target language for Google Translate.
googleTranslate.targetLang = en

# cat=GoogleTranslate; type=string; label=Google Translate Source Language: The source language for Google Translate (optional).
googleTranslate.sourceLang =

# cat=GoogleTranslate; type=options[html,text]; label=Google Translate Format: The format of the text to be translated.
googleTranslate.format = html

# cat=GoogleTranslate; type=string; label=Google Translate Model: The model to use for translation (optional).
googleTranslate.model =

# cat=GoogleTranslate; type=string; label=Google Translate Custom Model ID: The custom model ID to use for translation (optional).
googleTranslate.cid =

# cat=Solr; type=boolean; label=Enable Highlighting: If checked, Solr will return highlighted snippets instead of the full content.
solr.hl_enabled = 1

# cat=Solr; type=string; label=Highlighting Fields: Comma-separated list of fields to generate snippets from.
solr.hl_fl = contenu

# cat=Solr; type=int; label=Number of Snippets: The maximum number of highlighted snippets to generate per field.
solr.hl_snippets = 3

# cat=Solr; type=int; label=Fragment Size: The approximate size, in characters, of the snippets.
solr.hl_fragsize = 1000

# cat=Solr; type=boolean; label=Merge Contiguous Fragments: If enabled, adjacent snippets are merged into one.
solr.hl_mergeContiguous = 1
